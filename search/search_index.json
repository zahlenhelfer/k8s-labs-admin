{"config":{"lang":["en","de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"nav/","title":"Nav","text":"<ul> <li>Frob</li> <li>Setup a Cluster</li> <li>Setup an Ingress Controller</li> </ul>"},{"location":"cicd/argocd/","title":"Kubernetes Exercise: Deploy a Sample Application Using Argo CD (GitOps)","text":""},{"location":"cicd/argocd/#objectives","title":"Objectives","text":"<p>By the end of this exercise, you will:</p> <ol> <li>Install Argo CD on a Kubernetes cluster.</li> <li>Deploy a sample application from a GitHub repository using Argo CD.</li> <li>Access and verify the deployed application.</li> </ol>"},{"location":"cicd/argocd/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, kind, or cloud-based)</li> <li><code>kubectl</code> installed and configured</li> <li><code>argocd</code> CLI installed (<code>brew install argocd</code> or via https://argo-cd.readthedocs.io/en/stable/cli_installation/)</li> <li>Access to GitHub (optional: your own repo)</li> </ul>"},{"location":"cicd/argocd/#step-1-install-argo-cd","title":"Step 1: Install Argo CD","text":"<pre><code>kubectl create namespace argocd\n\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p>Wait for the pods to become ready:</p> <pre><code>kubectl get pods -n argocd\n</code></pre>"},{"location":"cicd/argocd/#step-2-access-the-argo-cd-ui","title":"Step 2: Access the Argo CD UI","text":""},{"location":"cicd/argocd/#option-1-port-forward-the-argo-cd-api-server","title":"Option 1: Port-forward the Argo CD API server","text":"<pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre> <p>Now access it via: https://localhost:8080</p>"},{"location":"cicd/argocd/#step-3-login-to-argo-cd","title":"Step 3: Login to Argo CD","text":"<p>Get the initial admin password:</p> <pre><code>kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath=\"{.data.password}\" | base64 -d &amp;&amp; echo\n</code></pre> <p>Then log in via CLI:</p> <pre><code>argocd login localhost:8080 --username admin --password &lt;copied-password&gt; --insecure\n</code></pre>"},{"location":"cicd/argocd/#step-4-deploy-a-sample-application","title":"Step 4: Deploy a Sample Application","text":"<p>We'll use a public GitHub repo with a simple nginx deployment:</p> <p>Repo: <code>https://github.com/argoproj/argocd-example-apps.git</code> Path: <code>guestbook</code></p>"},{"location":"cicd/argocd/#create-a-new-namespace-for-the-app","title":"Create a new namespace for the app","text":"<pre><code>kubectl create namespace guestbook\n</code></pre>"},{"location":"cicd/argocd/#create-an-argo-cd-application-manifest-guestbook-appyaml","title":"Create an Argo CD Application manifest (<code>guestbook-app.yaml</code>)","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\n  namespace: argocd\nspec:\n  destination:\n    namespace: guestbook\n    server: https://kubernetes.default.svc\n  project: default\n  source:\n    repoURL: https://github.com/argoproj/argocd-example-apps.git\n    targetRevision: HEAD\n    path: guestbook\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f guestbook-app.yaml\n</code></pre>"},{"location":"cicd/argocd/#step-5-verify-the-deployment","title":"Step 5: Verify the Deployment","text":"<p>Check the application in Argo CD UI or via CLI:</p> <pre><code>argocd app list\nargocd app get guestbook\n</code></pre> <p>Make sure the sync status is <code>Synced</code> and health is <code>Healthy</code>.</p>"},{"location":"cicd/argocd/#step-6-access-the-application","title":"Step 6: Access the Application","text":"<p>Port-forward the frontend service:</p> <pre><code>kubectl port-forward svc/guestbook-ui -n guestbook 8081:80\n</code></pre> <p>Visit: http://localhost:8081</p> <p>You should see the Guestbook UI running!</p>"},{"location":"compliance/kyverno/kyverno-labels/","title":"Lab: Deploy Kyverno and Enforce <code>costcenter</code> Label on Pods","text":""},{"location":"compliance/kyverno/kyverno-labels/#objective","title":"Objective","text":"<ol> <li>Deploy Kyverno in your cluster</li> <li>Create a Kyverno policy that validates all pods have a <code>costcenter</code> label</li> <li>Test pod creation with and without the label</li> </ol>"},{"location":"compliance/kyverno/kyverno-labels/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, Kind, etc.)</li> <li><code>kubectl</code> installed</li> <li>Cluster-admin access</li> </ul>"},{"location":"compliance/kyverno/kyverno-labels/#lab-structure","title":"\ud83d\udcc1 Lab Structure","text":"<pre><code>kyverno-lab/\n\u251c\u2500\u2500 policies/\n\u2502   \u2514\u2500\u2500 require-costcenter-label.yaml\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 good-pod.yaml\n    \u2514\u2500\u2500 bad-pod.yaml\n</code></pre>"},{"location":"compliance/kyverno/kyverno-labels/#install-kyverno","title":"Install Kyverno","text":"<pre><code>kubectl create -f https://github.com/kyverno/kyverno/releases/latest/download/install.yaml\n</code></pre> <p>Verify it\u2019s running:</p> <pre><code>kubectl get pods -n kyverno\n</code></pre>"},{"location":"compliance/kyverno/kyverno-labels/#create-the-required-label-policy","title":"Create the Required Label Policy","text":"<p>\ud83d\udcc4 <code>policies/require-costcenter-label.yaml</code></p> <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-costcenter-label\nspec:\n  validationFailureAction: Enforce\n  rules:\n    - name: check-costcenter-label\n      match:\n        resources:\n          kinds:\n            - Pod\n      validate:\n        message: \"All pods must have a 'costcenter' label.\"\n        pattern:\n          metadata:\n            labels:\n              costcenter: \"?*\"\n</code></pre> <p>Apply the policy:</p> <pre><code>kubectl apply -f policies/require-costcenter-label.yaml\n</code></pre>"},{"location":"compliance/kyverno/kyverno-labels/#test-the-policy","title":"Test the Policy","text":""},{"location":"compliance/kyverno/kyverno-labels/#good-pod","title":"Good Pod","text":"<p>\ud83d\udcc4 <code>test/good-pod.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: good-nginx\n  labels:\n    costcenter: devops\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f test/good-pod.yaml\n</code></pre> <p>\ud83d\udfe2 Should succeed.</p>"},{"location":"compliance/kyverno/kyverno-labels/#bad-pod-missing-costcenter","title":"\u274c Bad Pod (Missing <code>costcenter</code>)","text":"<p>\ud83d\udcc4 <code>test/bad-pod.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: bad-nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f test/bad-pod.yaml\n</code></pre> <p>\ud83d\udd34 Should be rejected with:</p> <pre><code>Error from server: admission webhook ... denied the request: All pods must have a 'costcenter' label.\n</code></pre>"},{"location":"compliance/kyverno/kyverno-labels/#summary","title":"Summary","text":"Component Description Kyverno Validating admission controller ClusterPolicy Enforces label presence on pod creation <code>costcenter</code> label Required label to pass policy check"},{"location":"compliance/kyverno/kyverno-labels/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete -f test/\nkubectl delete -f policies/require-costcenter-label.yaml\nkubectl delete -f https://github.com/kyverno/kyverno/releases/latest/download/install.yaml\n</code></pre>"},{"location":"crds/simple-crd/","title":"Lab: Create and Use a Custom Resource Definition (CRD)","text":""},{"location":"crds/simple-crd/#goal","title":"Goal","text":"<p>Create a <code>Fruit</code> CRD that allows you to define fruits like <code>apple</code>, <code>banana</code>, etc., and deploy a controller that prints them.</p>"},{"location":"crds/simple-crd/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (e.g. Minikube)</li> <li><code>kubectl</code> installed</li> <li>Optional: <code>kustomize</code> and <code>controller-gen</code> if building a full controller</li> </ul>"},{"location":"crds/simple-crd/#lab-structure","title":"Lab Structure","text":"<pre><code>crd-lab/\n\u251c\u2500\u2500 crds/\n\u2502   \u2514\u2500\u2500 fruit-crd.yaml\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 apple.yaml\n\u2514\u2500\u2500 controller/\n    \u2514\u2500\u2500 fruit-controller.sh\n</code></pre>"},{"location":"crds/simple-crd/#define-the-crd","title":"Define the CRD","text":"<p>\ud83d\udcc4 <code>crds/fruit-crd.yaml</code></p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: fruits.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                color:\n                  type: string\n                taste:\n                  type: string\n      subresources:\n        status: {}\n  scope: Namespaced\n  names:\n    plural: fruits\n    singular: fruit\n    kind: Fruit\n    shortNames:\n      - frt\n</code></pre> <p>Apply the CRD:</p> <pre><code>kubectl apply -f crds/fruit-crd.yaml\n</code></pre>"},{"location":"crds/simple-crd/#create-a-custom-resource-cr","title":"Create a Custom Resource (CR)","text":"<p>\ud83d\udcc4 <code>examples/apple.yaml</code></p> <pre><code>apiVersion: example.com/v1\nkind: Fruit\nmetadata:\n  name: apple\nspec:\n  color: red\n  taste: sweet\n</code></pre> <p>Apply the CR:</p> <pre><code>kubectl apply -f examples/apple.yaml\n</code></pre> <p>Verify:</p> <pre><code>kubectl get fruits\nkubectl get fruit apple -o yaml\n</code></pre>"},{"location":"crds/simple-crd/#create-a-simple-controller-optional","title":"Create a Simple Controller (optional)","text":"<p>This is a demo controller (bash script) that watches the fruit CRs and prints them. In real apps you'd use a proper controller with <code>client-go</code> or Kubebuilder.</p> <p>\ud83d\udcc4 <code>controller/fruit-controller.sh</code></p> <pre><code>#!/bin/bash\n\necho \"Watching for Fruit resources...\"\nkubectl get fruit --watch -o json | jq -c '.'\n</code></pre> <p>Run:</p> <pre><code>bash controller/fruit-controller.sh\n</code></pre> <p>Then try creating more fruits:</p> <pre><code># examples/banana.yaml\napiVersion: example.com/v1\nkind: Fruit\nmetadata:\n  name: banana\nspec:\n  color: yellow\n  taste: sweet\n</code></pre> <pre><code>kubectl apply -f examples/banana.yaml\n</code></pre>"},{"location":"crds/simple-crd/#clean-up","title":"Clean Up","text":"<pre><code>kubectl delete -f examples/\nkubectl delete -f crds/\n</code></pre>"},{"location":"crds/simple-crd/#summary","title":"Summary","text":"Component Description <code>Fruit</code> CRD Defines the structure of a fruit Custom Resources Concrete fruit instances Controller Watches and prints changes"},{"location":"kustomize/kustomize/","title":"Lab: Using Kustomize with Kubernetes","text":""},{"location":"kustomize/kustomize/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (Minikube, Kind, or any cluster)</li> <li><code>kubectl</code> installed</li> <li><code>kustomize</code> installed (or use <code>kubectl kustomize</code>)</li> </ul>"},{"location":"kustomize/kustomize/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<pre><code>kustomize-lab/\n\u251c\u2500\u2500 base/\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 overlays/\n    \u251c\u2500\u2500 dev/\n    \u2502   \u251c\u2500\u2500 kustomization.yaml\n    \u2502   \u2514\u2500\u2500 patch.yaml\n    \u2514\u2500\u2500 prod/\n        \u251c\u2500\u2500 kustomization.yaml\n        \u2514\u2500\u2500 patch.yaml\n</code></pre>"},{"location":"kustomize/kustomize/#create-the-base-configuration","title":"Create the Base Configuration","text":"<p>\ud83d\udcc4 <code>base/deployment.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.21\n          ports:\n            - containerPort: 80\n</code></pre> <p>\ud83d\udcc4 <code>base/service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>\ud83d\udcc4 <code>base/kustomization.yaml</code></p> <pre><code>resources:\n  - deployment.yaml\n  - service.yaml\n</code></pre>"},{"location":"kustomize/kustomize/#create-overlays","title":"Create Overlays","text":""},{"location":"kustomize/kustomize/#dev-overlay","title":"Dev Overlay","text":"<p>\ud83d\udcc4 <code>overlays/dev/kustomization.yaml</code></p> <pre><code>resources:\n  - ../../base\n\npatches:\n  - path: patch.yaml\n</code></pre> <p>\ud83d\udcc4 <code>overlays/dev/patch.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n</code></pre>"},{"location":"kustomize/kustomize/#prod-overlay","title":"Prod Overlay","text":"<p>\ud83d\udcc4 <code>overlays/prod/kustomization.yaml</code></p> <pre><code>resources:\n  - ../../base\n\npatches:\n  - path: patch.yaml\n</code></pre> <p>\ud83d\udcc4 <code>overlays/prod/patch.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n</code></pre>"},{"location":"kustomize/kustomize/#apply-the-configurations","title":"Apply the Configurations","text":""},{"location":"kustomize/kustomize/#dev","title":"Dev","text":"<pre><code>kubectl apply -k overlays/dev\n</code></pre>"},{"location":"kustomize/kustomize/#prod","title":"Prod","text":"<pre><code>kubectl apply -k overlays/prod\n</code></pre>"},{"location":"kustomize/kustomize/#verify","title":"Verify","text":"<pre><code>kubectl get deployments\nkubectl get services\n</code></pre> <p>You should see an <code>nginx</code> deployment with the correct number of replicas depending on the environment.</p>"},{"location":"kustomize/kustomize/#clean-up","title":"Clean Up","text":"<pre><code>kubectl delete -k overlays/dev\n# or\nkubectl delete -k overlays/prod\n</code></pre>"},{"location":"oberservability/logging/loki-stack/","title":"Kubernetes Exercise: Deploy Loki Stack with Log Collector and Sample App","text":""},{"location":"oberservability/logging/loki-stack/#objectives","title":"Objectives","text":"<p>By the end of this exercise, you will:</p> <ol> <li>Deploy the Loki Stack (Loki, Promtail, and Grafana) using Helm.</li> <li>Deploy a sample application that emits logs.</li> <li>View the application logs in Grafana using LogQL.</li> </ol>"},{"location":"oberservability/logging/loki-stack/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster (minikube, kind, cloud, etc.)</li> <li><code>kubectl</code> installed and configured</li> <li><code>helm</code> installed (https://helm.sh/docs/intro/install/)</li> <li>Optional: port 3000 open on localhost for Grafana</li> </ul>"},{"location":"oberservability/logging/loki-stack/#step-1-add-and-install-loki-stack-with-helm","title":"Step 1: Add and Install Loki Stack with Helm","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\n# Create a monitoring namespace\nkubectl create namespace monitoring\n\n# Install the Loki stack (Loki + Promtail + Grafana)\nhelm install loki-stack grafana/loki-stack --namespace monitoring \\\n  --set grafana.enabled=true \\\n  --set promtail.enabled=true \\\n  --set loki.persistence.enabled=false \\\n  --set grafana.service.type=NodePort\n</code></pre> <p>Wait until everything is ready:</p> <pre><code>kubectl get pods -n monitoring\n</code></pre>"},{"location":"oberservability/logging/loki-stack/#step-2-access-grafana-ui","title":"Step 2: Access Grafana UI","text":"<p>Port forward Grafana:</p> <pre><code>kubectl port-forward -n monitoring service/loki-stack-grafana 3000:80\n</code></pre> <p>Then open: http://localhost:3000</p> <p>Login credentials:</p> <ul> <li>User: <code>admin</code></li> <li>Password: <code>prom-operator</code> (default for Loki Helm chart)</li> </ul>"},{"location":"oberservability/logging/loki-stack/#step-3-deploy-a-sample-app-with-logs","title":"Step 3: Deploy a Sample App with Logs","text":"<p>Here\u2019s a simple \"log generator\" app that prints messages to stdout.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-emitter\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-emitter\n  template:\n    metadata:\n      labels:\n        app: log-emitter\n    spec:\n      containers:\n      - name: logger\n        image: busybox\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo Hello from $(hostname); sleep 5; done\"]\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f log-emitter.yaml\n</code></pre>"},{"location":"oberservability/logging/loki-stack/#step-4-view-logs-in-grafana","title":"Step 4: View Logs in Grafana","text":""},{"location":"oberservability/logging/loki-stack/#add-the-loki-data-source-if-not-already-added","title":"Add the Loki data source (if not already added)","text":"<ol> <li>Go to Grafana UI \u2192 \u2699\ufe0f Configuration \u2192 Data Sources</li> <li>Click Add data source \u2192 choose Loki</li> <li>URL: <code>http://loki-stack:3100</code></li> <li>Click Save &amp; Test</li> </ol>"},{"location":"oberservability/logging/loki-stack/#explore-logs","title":"Explore logs","text":"<ol> <li>Go to Explore</li> <li>Select Loki as the datasource</li> <li>Use a query like:</li> </ol> <pre><code>{app=\"log-emitter\"}\n</code></pre> <p>You should see logs like:</p> <pre><code>Hello from log-emitter-xxxx\n</code></pre>"},{"location":"oberservability/logging/loki-stack/#bonus-tasks","title":"Bonus Tasks","text":"<ul> <li>Scale the deployment to <code>replicas: 3</code> and observe the logs.</li> <li>Modify the log message (e.g., include timestamps or random IDs).</li> <li>Add a <code>label</code> to logs and filter by it in LogQL.</li> </ul>"},{"location":"oberservability/logging/loki-stack/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete deployment log-emitter\nhelm uninstall loki-stack -n monitoring\nkubectl delete namespace monitoring\n</code></pre>"},{"location":"oberservability/monitoring/metrics-server/","title":"Lab: Kubernetes Metrics Pipeline","text":"<p>Goal: Deploy <code>metrics-server</code> \u2192 Scrape with <code>Prometheus</code> \u2192 Visualize in <code>Grafana</code></p>"},{"location":"oberservability/monitoring/metrics-server/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, Kind, or similar)</li> <li><code>kubectl</code> and optionally <code>helm</code> installed</li> </ul>"},{"location":"oberservability/monitoring/metrics-server/#overview","title":"Overview","text":"<pre><code>+-------------------+\n| metrics-server    |  &lt;-- collects resource metrics (CPU/mem) from nodes and pods\n+-------------------+\n         \u2193\n+-------------------+\n| Prometheus        |  &lt;-- scrapes metrics-server via HTTP\n+-------------------+\n         \u2193\n+-------------------+\n| Grafana           |  &lt;-- visualizes Prometheus metrics\n+-------------------+\n</code></pre>"},{"location":"oberservability/monitoring/metrics-server/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"oberservability/monitoring/metrics-server/#deploy-metrics-server","title":"Deploy <code>metrics-server</code>","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> <p>\u26a0\ufe0f If you're using Minikube, patch the deployment:</p> <pre><code>kubectl patch deployment metrics-server -n kube-system \\\n  --type=json -p='[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]'\n</code></pre> <p>Confirm it's working:</p> <pre><code>kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\"\n</code></pre>"},{"location":"oberservability/monitoring/metrics-server/#install-prometheus-using-helm","title":"Install Prometheus using Helm","text":"<pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm install prometheus prometheus-community/prometheus \\\n  --namespace monitoring --create-namespace\n</code></pre> <p>Verify installation:</p> <pre><code>kubectl get pods -n monitoring\nkubectl port-forward svc/prometheus-server -n monitoring 9090:80\n</code></pre> <p>Visit: http://localhost:9090</p>"},{"location":"oberservability/monitoring/metrics-server/#configure-prometheus-to-scrape-metrics-server","title":"Configure Prometheus to Scrape Metrics Server","text":""},{"location":"oberservability/monitoring/metrics-server/#add-a-new-scrape-config","title":"Add a new scrape config","text":"<p>Create a configmap to add custom scrape configs:</p> <pre><code># metrics-scrape-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: additional-scrape-configs\n  namespace: monitoring\ndata:\n  scrape-configs.yaml: |\n    - job_name: 'metrics-server'\n      metrics_path: /metrics\n      scheme: https\n      static_configs:\n        - targets: ['metrics-server.kube-system.svc:443']\n      tls_config:\n        insecure_skip_verify: true\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f metrics-scrape-config.yaml\n</code></pre> <p>Now update the Prometheus release to use it:</p> <pre><code>helm upgrade prometheus prometheus-community/prometheus \\\n  --namespace monitoring \\\n  --set server.extraScrapeConfigsSecret.enabled=true \\\n  --set server.extraScrapeConfigsSecret.name=additional-scrape-configs \\\n  --set-file server.extraScrapeConfigsSecret.data.scrape-configs.yaml=metrics-scrape-config.yaml\n</code></pre>"},{"location":"oberservability/monitoring/metrics-server/#install-grafana-with-helm","title":"Install Grafana with Helm","text":"<pre><code>helm install grafana grafana/grafana \\\n  --namespace monitoring \\\n  --set adminPassword='admin' \\\n  --set service.type=NodePort \\\n  --create-namespace\n</code></pre> <p>Forward port:</p> <pre><code>kubectl port-forward svc/grafana -n monitoring 3000:80\n</code></pre> <p>Visit: http://localhost:3000 Login with: <code>admin / admin</code></p>"},{"location":"oberservability/monitoring/metrics-server/#configure-grafana","title":"Configure Grafana","text":"<ul> <li> <p>Add Prometheus as a data source:</p> </li> <li> <p>URL: <code>http://prometheus-server.monitoring.svc.cluster.local</code></p> </li> <li>Import dashboard (ID: 6417 for Kubernetes resource metrics)</li> </ul>"},{"location":"oberservability/monitoring/metrics-server/#done","title":"Done","text":"<p>Now you're visualizing real-time Kubernetes metrics via metrics-server \u2192 Prometheus \u2192 Grafana.</p>"},{"location":"oberservability/monitoring/metrics-server/#cleanup","title":"Cleanup","text":"<pre><code>helm uninstall prometheus -n monitoring\nhelm uninstall grafana -n monitoring\nkubectl delete ns monitoring\nkubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre>"},{"location":"oberservability/monitoring/metrics-tls/","title":"\ud83d\udd2c Lab: Deploy Metrics Server with Custom TLS SANs","text":""},{"location":"oberservability/monitoring/metrics-tls/#goal","title":"Goal","text":"<p>Deploy a working <code>metrics-server</code> instance with a valid certificate including the SANs needed to talk to the kubelets securely (<code>--kubelet-preferred-address-types=InternalIP,Hostname</code>).</p>"},{"location":"oberservability/monitoring/metrics-tls/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (Minikube, Kind, etc.)</li> <li><code>kubectl</code>, <code>openssl</code></li> <li>Cluster admin permissions</li> </ul>"},{"location":"oberservability/monitoring/metrics-tls/#lab-steps-overview","title":"Lab Steps Overview","text":"<ol> <li>Generate a TLS certificate with proper SANs</li> <li>Patch or install <code>metrics-server</code> using the new certificate</li> <li>Test metrics collection with <code>kubectl top</code></li> </ol>"},{"location":"oberservability/monitoring/metrics-tls/#generate-certificate-with-required-sans","title":"Generate Certificate with Required SANs","text":"<pre><code>mkdir -p metrics-server-cert &amp;&amp; cd metrics-server-cert\n</code></pre> <p>\ud83d\udcc4 <code>csr.conf</code></p> <pre><code>[req]\nreq_extensions = v3_req\ndistinguished_name = req_distinguished_name\nprompt = no\n\n[req_distinguished_name]\nCN = metrics-server\n\n[v3_req]\nbasicConstraints = CA:FALSE\nkeyUsage = digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = metrics-server\nDNS.2 = metrics-server.kube-system\nDNS.3 = metrics-server.kube-system.svc\n</code></pre> <p>Generate cert:</p> <pre><code>openssl genrsa -out metrics-server.key 2048\n\nopenssl req -new -key metrics-server.key -out metrics-server.csr -config csr.conf\n\nopenssl x509 -req -in metrics-server.csr \\\n  -signkey metrics-server.key \\\n  -out metrics-server.crt -days 365 \\\n  -extensions v3_req -extfile csr.conf\n</code></pre> <p>You now have:</p> <ul> <li><code>metrics-server.crt</code> (signed cert)</li> <li><code>metrics-server.key</code> (private key)</li> </ul>"},{"location":"oberservability/monitoring/metrics-tls/#deploy-metrics-server-with-custom-tls","title":"Deploy Metrics Server with Custom TLS","text":"<p>Download and edit the deployment YAML:</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml --dry-run=client -o yaml &gt; metrics-server.yaml\n</code></pre>"},{"location":"oberservability/monitoring/metrics-tls/#modify-metrics-serveryaml","title":"Modify <code>metrics-server.yaml</code>","text":"<ul> <li>Add volume mounts for the certificate</li> <li>Add <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags</li> <li>Optionally: <code>--kubelet-insecure-tls=false</code> if SANs are correct</li> </ul> <p>\ud83d\udccc Example patch to Deployment:</p> <pre><code>spec:\n  containers:\n    - name: metrics-server\n      image: ...\n      args:\n        - --cert-dir=/certs\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP,Hostname\n        - --tls-cert-file=/certs/metrics-server.crt\n        - --tls-private-key-file=/certs/metrics-server.key\n      volumeMounts:\n        - name: certs\n          mountPath: /certs\n          readOnly: true\n  volumes:\n    - name: certs\n      secret:\n        secretName: metrics-server-certs\n</code></pre>"},{"location":"oberservability/monitoring/metrics-tls/#create-the-certificate-as-a-secret","title":"Create the Certificate as a Secret","text":"<pre><code>kubectl create secret generic metrics-server-certs \\\n  --from-file=metrics-server.crt=metrics-server.crt \\\n  --from-file=metrics-server.key=metrics-server.key \\\n  -n kube-system\n</code></pre>"},{"location":"oberservability/monitoring/metrics-tls/#apply-the-modified-deployment","title":"Apply the Modified Deployment","text":"<pre><code>kubectl apply -f metrics-server.yaml\n</code></pre>"},{"location":"oberservability/monitoring/metrics-tls/#verify","title":"Verify","text":"<p>Wait for the pod to be ready:</p> <pre><code>kubectl get pods -n kube-system | grep metrics-server\n</code></pre> <p>Then check metrics:</p> <pre><code>kubectl top nodes\nkubectl top pods --all-namespaces\n</code></pre> <p>If you still get TLS or SAN errors, double-check the cert SAN entries match the service DNS.</p>"},{"location":"oberservability/monitoring/metrics-tls/#optional-cleanup","title":"Optional Cleanup","text":"<pre><code>kubectl delete -f metrics-server.yaml\nkubectl delete secret metrics-server-certs -n kube-system\n</code></pre>"},{"location":"security/firewall-ansible/","title":"\ud83e\uddea Exercise: Firewall Setup with Ansible-Automation","text":"<p>Here\u2019s a complete Ansible playbook that configures a basic UFW firewall for a Kubernetes cluster with:</p>"},{"location":"security/firewall-ansible/#objective","title":"\ud83d\udcdd Objective","text":"<p>You will learn to:</p> <ul> <li>Install and enable the UFW firewall via Ansible on:</li> <li>One control-plane node</li> <li>One or more worker nodes</li> <li>assign roles (control-plane or worker) using host groups in your Ansible inventory.</li> </ul>"},{"location":"security/firewall-ansible/#directory-structure-recommended","title":"\ud83d\udcc1 Directory structure (recommended)","text":"<pre><code>k8s-firewall/\n\u251c\u2500\u2500 inventory.ini\n\u251c\u2500\u2500 playbook.yml\n</code></pre>"},{"location":"security/firewall-ansible/#inventoryini","title":"\ud83d\udcd8 inventory.ini","text":"<pre><code>[control_plane]\nmaster1 ansible_host=192.168.0.10\n\n[worker]\nworker1 ansible_host=192.168.0.11\nworker2 ansible_host=192.168.0.12\n</code></pre>"},{"location":"security/firewall-ansible/#playbookyml","title":"\ud83d\udcdc playbook.yml","text":"<pre><code>---\n- name: Configure UFW firewall for Kubernetes nodes\n  hosts: all\n  become: true\n  vars:\n    internal_subnet: \"192.168.0.0/24\"\n\n  tasks:\n    - name: Install UFW\n      apt:\n        name: ufw\n        state: present\n        update_cache: yes\n\n    - name: Set default policies\n      ufw:\n        direction: incoming\n        policy: deny\n\n    - name: Allow outgoing traffic\n      ufw:\n        direction: outgoing\n        policy: allow\n\n    - name: Allow SSH\n      ufw:\n        rule: allow\n        port: 22\n        proto: tcp\n\n- name: Configure control-plane firewall rules\n  hosts: control_plane\n  become: true\n  tasks:\n    - name: Allow Kubernetes API server\n      ufw:\n        rule: allow\n        port: 6443\n        proto: tcp\n\n    - name: Allow etcd ports\n      ufw:\n        rule: allow\n        port: \"{{ item }}\"\n        proto: tcp\n      loop: [2379, 2380]\n\n    - name: Allow kubelet API\n      ufw:\n        rule: allow\n        port: 10250\n        proto: tcp\n\n    - name: Allow scheduler and controller-manager\n      ufw:\n        rule: allow\n        port: \"{{ item }}\"\n        proto: tcp\n      loop: [10257, 10259]\n\n    - name: Enable UFW\n      ufw:\n        state: enabled\n\n- name: Configure worker node firewall rules\n  hosts: worker\n  become: true\n  tasks:\n    - name: Allow kubelet API\n      ufw:\n        rule: allow\n        port: 10250\n        proto: tcp\n\n    - name: Allow NodePort range\n      ufw:\n        rule: allow\n        port: \"30000:32767\"\n        proto: tcp\n\n    - name: Enable UFW\n      ufw:\n        state: enabled\n</code></pre>"},{"location":"security/firewall-ansible/#run-the-playbook","title":"\u25b6\ufe0f Run the playbook","text":"<pre><code>ansible-playbook -i inventory.ini playbook.yml\n</code></pre> <p>\ud83d\udd10 Use <code>--ask-become-pass</code> if you don\u2019t use passwordless sudo.</p>"},{"location":"security/firewall-ansible/#outcome","title":"\u2705 Outcome","text":"<ul> <li>UFW will be enabled on all nodes</li> <li>Only the necessary Kubernetes ports will be open</li> <li>Everything else will be blocked</li> </ul>"},{"location":"security/firewall-ufw/","title":"Exercise: Install and Configure Firewall for a Kubernetes Cluster (Ubuntu 24.04)","text":""},{"location":"security/firewall-ufw/#objective","title":"\ud83d\udcdd Objective","text":"<p>You will learn to:</p> <ul> <li>Install and enable the UFW firewall</li> <li>Allow only essential ports for Kubernetes</li> <li>Block all unnecessary traffic</li> <li>Set up rules for a secure single control-plane and worker node setup</li> </ul>"},{"location":"security/firewall-ufw/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li> <p>Two Ubuntu 24.04 servers:</p> </li> <li> <p><code>control-plane</code> (master) node</p> </li> <li><code>worker</code> node</li> <li>Root/sudo access</li> <li>Kubernetes not yet initialized (optional, this can be done before or after firewall setup)</li> </ul>"},{"location":"security/firewall-ufw/#part-1-install-and-enable-ufw","title":"\ud83e\udde9 Part 1: Install and Enable UFW","text":"<p>On both nodes:</p> <pre><code>sudo apt update\nsudo apt install -y ufw\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\n</code></pre>"},{"location":"security/firewall-ufw/#part-2-configure-firewall-rules-for-kubernetes","title":"\ud83e\udde9 Part 2: Configure Firewall Rules for Kubernetes","text":""},{"location":"security/firewall-ufw/#on-the-control-plane-node","title":"\ud83d\udea9 On the Control-Plane Node","text":"<p>These ports must be allowed:</p> Port Protocol Purpose 6443 TCP Kubernetes API server 2379-2380 TCP etcd (cluster store) 10250 TCP Kubelet API 10259 TCP kube-scheduler 10257 TCP kube-controller-manager <pre><code># Allow SSH\nsudo ufw allow 22/tcp\n\n# Allow Kubernetes Control Plane ports\nsudo ufw allow 6443/tcp\nsudo ufw allow 2379:2380/tcp\nsudo ufw allow 10250/tcp\nsudo ufw allow 10257/tcp\nsudo ufw allow 10259/tcp\n</code></pre> <p>\u2705 Optional: Restrict access to only internal IP range (e.g. <code>192.168.0.0/24</code>):</p> <pre><code>sudo ufw allow from 192.168.0.0/24 to any port 6443 proto tcp\n</code></pre>"},{"location":"security/firewall-ufw/#on-the-worker-node","title":"\ud83d\udea9 On the Worker Node","text":"<p>These ports must be allowed:</p> Port Protocol Purpose 10250 TCP Kubelet API 30000\u201332767 TCP NodePort Services (optional) 6783 TCP/UDP (if using Weave Net) <pre><code># Allow SSH\nsudo ufw allow 22/tcp\n\n# Allow Kubernetes ports\nsudo ufw allow 10250/tcp\nsudo ufw allow 30000:32767/tcp\n</code></pre> <p>If you're using container network plugins (CNI) like Flannel, Calico, or Cilium, open their ports too:</p> <p>Example (Flannel):</p> <pre><code>sudo ufw allow 8285/udp\nsudo ufw allow 8472/udp\n</code></pre> <p>Example (Calico):</p> <pre><code>sudo ufw allow 179/tcp\nsudo ufw allow 4789/udp\n</code></pre> <p>Example (Cilium with kube-proxy):</p> <pre><code># VXLAN overlay (default)\nsudo ufw allow 8472/udp\n\n# Health checks (agent &lt;-&gt; agent)\nsudo ufw allow 4240/tcp\n\n# Cilium agent API (optional debug)\nsudo ufw allow 4244/tcp\n</code></pre> <p>Example (Cilium without kube-proxy):</p> <pre><code># VXLAN overlay (or Geneve if configured)\nsudo ufw allow 8472/udp\n\n# Health checks between nodes\nsudo ufw allow 4240/tcp\n\n# Cilium agent API (optional debug)\nsudo ufw allow 4244/tcp\n\n# Cilium HostPort/ClusterIP load-balancing (BPF-based)\nsudo ufw allow 6081/udp     # if using Geneve encapsulation instead of VXLAN\nsudo ufw allow 179/tcp      # if using BGP (optional)\n</code></pre>"},{"location":"security/firewall-ufw/#part-3-enable-ufw","title":"\ud83d\ude80 Part 3: Enable UFW","text":"<p>On both nodes:</p> <pre><code>sudo ufw enable\n</code></pre> <p>Confirm with <code>y</code> when asked.</p>"},{"location":"security/firewall-ufw/#part-4-verify-rules","title":"\ud83d\udd0d Part 4: Verify Rules","text":"<pre><code>sudo ufw status numbered\n</code></pre>"},{"location":"security/firewall-ufw/#part-5-optional-restrict-access-further","title":"\ud83d\udd10 Part 5: Optional - Restrict Access Further","text":"<ul> <li>You can restrict API server access to only certain IPs:</li> </ul> <pre><code>sudo ufw delete allow 6443/tcp\nsudo ufw allow from 192.168.0.10 to any port 6443 proto tcp\n</code></pre>"},{"location":"security/firewall-ufw/#part-6-testing","title":"\ud83e\uddea Part 6: Testing","text":"<ul> <li>Try accessing blocked ports (e.g. 8080) with <code>telnet</code> or <code>nc</code>:</li> </ul> <pre><code>nc -zv &lt;target-ip&gt; 8080\n</code></pre> <ul> <li>Try accessing allowed ports (e.g. 6443):</li> </ul> <pre><code>nc -zv &lt;control-plane-ip&gt; 6443\n</code></pre>"},{"location":"security/firewall-ufw/#summary-of-required-kubernetes-ports","title":"\ud83e\uddfe Summary of Required Kubernetes Ports","text":"Component Control Plane Worker Node SSH 22 22 Kubernetes API server 6443 \u2717 etcd 2379\u20132380 \u2717 Kubelet API 10250 10250 kube-scheduler 10259 \u2717 kube-controller-manager 10257 \u2717 NodePort services Optional 30000\u201332767"},{"location":"security/kubeconfig/","title":"\ud83d\udd2c Lab: Create a Kubernetes User and Generate <code>kubeconfig</code>","text":""},{"location":"security/kubeconfig/#goal","title":"\ud83c\udfaf Goal","text":"<p>Create a user <code>developer</code> using a client certificate and give them access to a specific namespace using RBAC.</p>"},{"location":"security/kubeconfig/#prerequisites","title":"\ud83e\uddf0 Prerequisites","text":"<ul> <li>A working Kubernetes cluster (e.g. Minikube, Kind)</li> <li><code>openssl</code>, <code>kubectl</code>, and <code>cfssl</code> (optional) installed</li> <li>Cluster admin privileges (to create certs, roles, and secrets)</li> </ul>"},{"location":"security/kubeconfig/#lab-structure","title":"\ud83d\udcc1 Lab Structure","text":"<pre><code>developer-lab/\n\u251c\u2500\u2500 certs/\n\u2502   \u251c\u2500\u2500 developer-csr.conf\n\u2502   \u251c\u2500\u2500 developer.csr\n\u2502   \u251c\u2500\u2500 developer.key\n\u2502   \u2514\u2500\u2500 developer.crt\n\u251c\u2500\u2500 kubeconfig/\n\u2502   \u2514\u2500\u2500 developer.kubeconfig\n\u2514\u2500\u2500 rbac/\n    \u2514\u2500\u2500 developer-role.yaml\n</code></pre>"},{"location":"security/kubeconfig/#1-generate-certificate-for-the-user","title":"1\ufe0f\u20e3 Generate Certificate for the User","text":"<pre><code>mkdir -p developer-lab/certs &amp;&amp; cd developer-lab/certs\n</code></pre> <p>\ud83d\udcc4 <code>developer-csr.conf</code></p> <pre><code>[req]\ndefault_bits = 2048\nprompt = no\ndefault_md = sha256\ndistinguished_name = dn\n\n[dn]\nCN = developer\nO = dev-team\n</code></pre> <p>Now generate:</p> <pre><code>openssl genrsa -out developer.key 2048\n\nopenssl req -new -key developer.key \\\n  -out developer.csr -config developer-csr.conf\n\n# Approve and sign with cluster CA (update path as needed)\nopenssl x509 -req -in developer.csr \\\n  -CA /etc/kubernetes/pki/ca.crt \\\n  -CAkey /etc/kubernetes/pki/ca.key \\\n  -CAcreateserial \\\n  -out developer.crt -days 365\n</code></pre> <p>\ud83d\udd10 You now have <code>developer.crt</code> and <code>developer.key</code>.</p>"},{"location":"security/kubeconfig/#2-create-a-kubeconfig-for-the-developer","title":"2\ufe0f\u20e3 Create a <code>kubeconfig</code> for the Developer","text":"<pre><code>cd ../kubeconfig\nCLUSTER_NAME=$(kubectl config view --minify -o jsonpath='{.clusters[0].name}')\nCLUSTER_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\nCA_CERT=$(kubectl config view --raw --minify -o jsonpath='{.clusters[0].cluster.certificate-authority-data}')\n\nkubectl config set-cluster \"$CLUSTER_NAME\" \\\n  --certificate-authority=/etc/kubernetes/pki/ca.crt \\\n  --embed-certs=true \\\n  --server=\"$CLUSTER_SERVER\" \\\n  --kubeconfig=developer.kubeconfig\n\nkubectl config set-credentials developer \\\n  --client-certificate=../certs/developer.crt \\\n  --client-key=../certs/developer.key \\\n  --embed-certs=true \\\n  --kubeconfig=developer.kubeconfig\n\nkubectl config set-context developer-context \\\n  --cluster=\"$CLUSTER_NAME\" \\\n  --namespace=dev \\\n  --user=developer \\\n  --kubeconfig=developer.kubeconfig\n\nkubectl config use-context developer-context --kubeconfig=developer.kubeconfig\n</code></pre> <p>\u2705 <code>developer.kubeconfig</code> is now ready.</p>"},{"location":"security/kubeconfig/#3-create-namespace-and-rbac-for-the-user","title":"3\ufe0f\u20e3 Create Namespace and RBAC for the User","text":"<pre><code>kubectl create namespace dev\n</code></pre> <p>\ud83d\udcc4 <code>rbac/developer-role.yaml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: dev\n  name: developer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f rbac/developer-role.yaml\n</code></pre>"},{"location":"security/kubeconfig/#4-test-as-developer","title":"4\ufe0f\u20e3 Test as Developer","text":"<p>Use the new <code>kubeconfig</code>:</p> <pre><code>kubectl --kubeconfig=developer.kubeconfig get pods\n</code></pre> <p>Try creating a pod:</p> <pre><code>kubectl --kubeconfig=developer.kubeconfig run nginx --image=nginx\n</code></pre> <p>\u2705 Success! You\u2019re using Kubernetes as the <code>developer</code> user.</p>"},{"location":"security/kubeconfig/#cleanup","title":"\ud83e\uddf9 Cleanup","text":"<pre><code>kubectl delete ns dev\nkubectl delete -f rbac/developer-role.yaml\n</code></pre>"},{"location":"security/nmap/","title":"Exercise: Nmap Scan of a Kubernetes Server on Ubuntu 24.04","text":""},{"location":"security/nmap/#objective","title":"Objective","text":"<p>By the end of this exercise, you will:</p> <ul> <li>Understand which services Kubernetes exposes by default</li> <li>Use Nmap to discover open ports and service versions</li> <li>Identify security-relevant information from the scan</li> </ul>"},{"location":"security/nmap/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>A Kubernetes cluster running (even a single-node cluster like <code>minikube</code> or <code>k3s</code>)</li> <li>A second Ubuntu 24.04 server to act as the scanning machine</li> <li>Root or sudo access on both</li> <li>Nmap installed on the scanning machine</li> </ul>"},{"location":"security/nmap/#part-1-install-nmap","title":"\ud83e\udde9 Part 1: Install Nmap","text":""},{"location":"security/nmap/#on-the-ubuntu-scanning-machine","title":"On the Ubuntu scanning machine","text":"<pre><code>sudo apt update\nsudo apt install -y nmap\n</code></pre>"},{"location":"security/nmap/#part-2-identify-target-information","title":"\ud83e\udde9 Part 2: Identify Target Information","text":""},{"location":"security/nmap/#1-get-the-ip-of-the-kubernetes-master-node","title":"1. Get the IP of the Kubernetes master node","text":"<p>On the Kubernetes server:</p> <pre><code>ip a\n</code></pre> <p>Note the IP address (e.g., <code>192.168.122.10</code>) to scan from the other machine.</p>"},{"location":"security/nmap/#part-3-perform-nmap-scans","title":"\ud83e\udde9 Part 3: Perform Nmap Scans","text":""},{"location":"security/nmap/#1-basic-port-scan-top-1000-ports","title":"1. Basic port scan (top 1000 ports)","text":"<pre><code>nmap 192.168.122.10\n</code></pre>"},{"location":"security/nmap/#nothing-should-happen-because-it-s-just-the-top-1000-ports","title":"Nothing should happen, because it\u00b4s just the top 1000 ports","text":""},{"location":"security/nmap/#2-full-port-scan-much-slower","title":"2. Full port scan (much slower)","text":"<pre><code>sudo nmap -p- 192.168.122.10\n</code></pre> <p>Expected open ports (for Kubernetes):</p> <ul> <li>6443: Kubernetes API server (HTTPS)</li> <li>10250: Kubelet API</li> <li>10255: (deprecated, read-only Kubelet API)</li> <li>10257/10259: Controller/Manager</li> <li>2379\u20132380: etcd ports</li> </ul>"},{"location":"security/nmap/#3-service-version-detection","title":"3. Service version detection","text":"<pre><code>sudo nmap -sV -p 6443,10250,2379,2380 192.168.122.10\n</code></pre> <p>This reveals software versions, e.g.:</p> <pre><code>PORT     STATE SERVICE  VERSION\n6443/tcp open  https     Kubernetes API server 1.28.2\n</code></pre>"},{"location":"security/nmap/#4-os-detection-optional","title":"4. OS detection (optional)","text":"<pre><code>sudo nmap -O 192.168.122.10\n</code></pre>"},{"location":"security/nmap/#5-scan-with-nmap-nse-scripts-for-ssltls","title":"5. Scan with Nmap NSE scripts for SSL/TLS","text":"<pre><code>sudo nmap --script ssl-cert,ssl-enum-ciphers -p 6443 192.168.122.10\n</code></pre> <p>This checks for:</p> <ul> <li>TLS version support</li> <li>Certificate details</li> <li>Weak ciphers</li> </ul>"},{"location":"security/nmap/#part-4-interpreting-results","title":"\ud83e\udde9 Part 4: Interpreting Results","text":"<p>Answer the following:</p> <ol> <li>What Kubernetes-related ports were open?</li> <li>Was the Kubernetes API secured (HTTPS/TLS)?</li> <li>Did any services reveal their version info?</li> <li>Was etcd exposed externally (risk!)?</li> </ol>"},{"location":"security/nmap/#part-5-hardening-suggestions-optional","title":"\ud83e\udde9 Part 5: Hardening Suggestions (Optional)","text":"<p>If etcd or the kubelet API is exposed:</p> <ul> <li>Restrict access via firewall</li> <li>Use authentication on exposed APIs</li> <li>Verify TLS configurations</li> </ul> <p>Example:</p> <pre><code>sudo ufw allow from 192.168.122.0/24 to any port 6443 proto tcp\n</code></pre>"},{"location":"security/nmap/#summary","title":"\ud83e\uddfe Summary","text":"Scan Type Command Purpose Basic Scan <code>nmap &lt;IP&gt;</code> Quick overview Full Port Scan <code>nmap -p- &lt;IP&gt;</code> All 65535 ports Version Scan <code>nmap -sV -p &lt;ports&gt; &lt;IP&gt;</code> Get service versions TLS Analysis <code>nmap --script ssl-* -p &lt;port&gt; &lt;IP&gt;</code> Analyze certificate + TLS support"},{"location":"security/suricata/","title":"\ud83e\uddea Exercise: Install, Configure, and Test Suricata on Ubuntu 24.04","text":""},{"location":"security/suricata/#objective","title":"\ud83d\udcdd Objective","text":"<p>By the end of this exercise, you will be able to:</p> <ul> <li>Install Suricata</li> <li>Configure basic rules</li> <li>Run Suricata in IDS mode</li> <li>Test Suricata with a simple network attack pattern</li> </ul>"},{"location":"security/suricata/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>A clean Ubuntu 24.04 server (bare metal or VM)</li> <li>Root or sudo access</li> <li>Internet connection</li> </ul>"},{"location":"security/suricata/#part-1-installation","title":"\ud83e\udde9 Part 1: Installation","text":""},{"location":"security/suricata/#1-update-the-system","title":"1. Update the system","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"security/suricata/#2-add-the-suricata-ppa-and-install","title":"2. Add the Suricata PPA and install","text":"<pre><code>sudo apt install -y software-properties-common\nsudo add-apt-repository ppa:oisf/suricata-stable\nsudo apt update\nsudo apt install -y suricata\n</code></pre>"},{"location":"security/suricata/#3-check-the-installation","title":"3. Check the installation","text":"<pre><code>suricata --build-info | grep 'Suricata version'\n</code></pre>"},{"location":"security/suricata/#part-2-configuration","title":"\u2699\ufe0f Part 2: Configuration","text":""},{"location":"security/suricata/#1-check-default-configuration-file","title":"1. Check default configuration file","text":"<pre><code>sudo nano /etc/suricata/suricata.yaml\n</code></pre> <p>Optional: Change the default interface (look for the <code>af-packet</code> section)</p> <pre><code>af-packet:\n  - interface: eth0   # Replace eth0 with your actual interface name\n</code></pre>"},{"location":"security/suricata/#2-identify-your-network-interface","title":"2. Identify your network interface","text":"<pre><code>ip a\n</code></pre> <p>Note your active interface (e.g. <code>ens33</code>, <code>eth0</code>, etc.)</p>"},{"location":"security/suricata/#part-3-test-rules","title":"\ud83e\uddea Part 3: Test Rules","text":""},{"location":"security/suricata/#1-download-default-rule-set","title":"1. Download default rule set","text":"<pre><code>sudo apt install -y suricata-update\nsudo suricata-update\n</code></pre>"},{"location":"security/suricata/#2-add-a-custom-test-rule","title":"2. Add a custom test rule","text":"<p>Create a test rule file:</p> <pre><code>sudo nano /etc/suricata/rules/local.rules\n</code></pre> <p>Paste this test rule:</p> <pre><code>alert icmp any any -&gt; any any (msg:\"ICMP Packet Detected\"; sid:1000001; rev:1;)\n</code></pre>"},{"location":"security/suricata/#3-edit-suricatayaml-to-enable-local-rules","title":"3. Edit suricata.yaml to enable local rules","text":"<p>Open the config file:</p> <pre><code>sudo nano /etc/suricata/suricata.yaml\n</code></pre> <p>Find and set the rule-files section:</p> <pre><code>rule-files:\n  - local.rules\n</code></pre> <p>Save and exit.</p>"},{"location":"security/suricata/#part-4-running-suricata","title":"\u25b6\ufe0f Part 4: Running Suricata","text":""},{"location":"security/suricata/#1-run-suricata-in-ids-mode","title":"1. Run Suricata in IDS mode","text":"<pre><code>sudo suricata -c /etc/suricata/suricata.yaml -i eth0\n</code></pre> <p>(Replace <code>eth0</code> with your actual interface)</p> <p>Keep this terminal running.</p>"},{"location":"security/suricata/#part-5-testing-suricata","title":"\ud83e\uddea Part 5: Testing Suricata","text":""},{"location":"security/suricata/#1-open-a-second-terminal-and-ping-any-host","title":"1. Open a second terminal and ping any host","text":"<pre><code>ping 8.8.8.8\n</code></pre> <p>Let it run for a few seconds, then stop it.</p>"},{"location":"security/suricata/#2-check-suricata-logs","title":"2. Check Suricata logs","text":"<pre><code>sudo tail -f /var/log/suricata/fast.log\n</code></pre> <p>You should see an alert similar to:</p> <pre><code>[**] [1:1000001:1] ICMP Packet Detected [**]\n</code></pre>"},{"location":"security/suricata/#part-6-cleanup-enable-as-a-service-optional","title":"\u2705 Part 6: Cleanup &amp; Enable as a Service (Optional)","text":"<p>If you want Suricata to start at boot:</p> <pre><code>sudo systemctl enable suricata\nsudo systemctl start suricata\n</code></pre> <p>To check the status:</p> <pre><code>sudo systemctl status suricata\n</code></pre>"},{"location":"security/suricata/#summary","title":"\ud83d\udccc Summary","text":"Step Description 1 Installed Suricata and rule sets 2 Configured interface and test rules 3 Ran Suricata in IDS mode 4 Verified alerts from test traffic"},{"location":"setup/cluster/","title":"Installation eines zwei Node Kubernetes-Cluster","text":""},{"location":"setup/cluster/#login","title":"Login","text":"<ul> <li><code>ssh student@k8s-node-XX.dockerlabs.de</code></li> </ul>"},{"location":"setup/cluster/#installation-der-control-plane","title":"Installation der Control-Plane","text":"<ul> <li>Login auf den ersten Server</li> <li>wechsel in das Verzeichnis: <code>cd LFD459/SOLUTIONS/s_02/</code></li> <li>starten des Scripts: <code>script -q -c \"bash k8scp.sh\" $HOME/controlplane.out</code></li> <li>bei der Frage <code>keep the local version currently installed</code> ausw\u00e4hlen</li> <li>Kopieren des Join-Befehls  aus der Log-Datei per grep: <code>grep -A1 \"kubeadm join\" controlplane.out</code> <li>Besipieloutput: <code>kubeadm join &lt;ip&gt;:6443 --token 1123 --discovery-token-ca-cert-hash sha256:16df252</code></li> <li>Installation der Auto-Completion:</li> <pre><code>source &lt;(kubectl completion bash)\necho \"source &lt;(kubectl completion bash)\" &gt;&gt; $HOME/.bashrc\n</code></pre> <ul> <li>Installation Pr\u00fcfen:</li> </ul> <pre><code>kubectl get nodes\nNAME         STATUS   ROLES           AGE   VERSION\nk8s-node-0   Ready    control-plane   11m   v1.33.1\n</code></pre> <ul> <li>Taint enfernen: <code>kubectl taint nodes --all node-role.kubernetes.io/control-plane-</code></li> </ul>"},{"location":"setup/cluster/#installation-des-worker","title":"Installation des Worker","text":"<ul> <li>Login auf den zweiten Server</li> <li>wechsel in das Verzeichnis: <code>cd LFD459/SOLUTIONS/s_02/</code></li> <li><code>bash k8sWorker.sh</code></li> <li><code>sudo kubeadm join ...</code></li> <li>wechsel zur Control-Plane und wiederholtes: <code>kubectl get nodes</code></li> </ul>"},{"location":"setup/cluster/#test","title":"Test","text":"<pre><code>graph LR\n    Worker-Node --&gt; ControlPlane\n    Worker-Node --&gt; ControlPlane\n    Kubectl &lt;-- ControlPlane\n</code></pre> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];\n</code></pre>"},{"location":"setup/ingress/","title":"\u00dcbung: NGINX-Ingress-Controller installieren","text":""},{"location":"setup/ingress/#schritt-1-anlegen-einer-valuesyaml-datei","title":"Schritt 1: Anlegen einer values.yaml Datei","text":"<pre><code>controller:\n  hostNetwork: true\n  kind: DaemonSet\n  ingressClassResource:\n    enabled: true\n    default: true\n  service:\n    externalIPs: [&lt;IP&gt;]\n\n</code></pre>"},{"location":"setup/ingress/#schritt-2-auslesen-der-externen-host-ip-adresse","title":"Schritt 2: Auslesen der externen HOST-IP Adresse","text":"<ul> <li> <p>Die IP Adresse kann einfach Shell ausgelesen werden: <code>ip -4 -o addr show scope global | awk '{print $4}' | cut -d/ -f1 | head -n1</code></p> </li> <li> <p>Ersetzen von <code>&lt;IP&gt;</code> in der <code>values.yaml</code></p> </li> </ul>"},{"location":"setup/ingress/#schritt-3-helm-installation-des-nginx-ingress-controller-inkl-valuesyaml","title":"Schritt 3: Helm Installation des NGINX-Ingress Controller inkl. <code>values.yaml</code>","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx &amp;&amp; helm repo update\nhelm install nginx-ingress ingress-nginx/ingress-nginx\\\n  --create-namespace --namespace ingress-controller\\\n  --version 4.12.2\\\n  -f values.yaml\n</code></pre>"},{"location":"storage/hostpath/","title":"Kubernetes <code>hostPath</code> Lab","text":""},{"location":"storage/hostpath/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (e.g. Minikube, Kind)</li> <li><code>kubectl</code> installed and configured</li> <li>Access to the node (for file verification)</li> </ul>"},{"location":"storage/hostpath/#step-1-prepare-the-host-directory","title":"Step 1: Prepare the host directory","text":"<p>Login to your Kubernetes node (if using Minikube, run: <code>minikube ssh</code>) and create a directory:</p> <pre><code>sudo mkdir -p /data/hostpath-test\nsudo chmod 777 /data/hostpath-test\n</code></pre>"},{"location":"storage/hostpath/#step-2-create-a-pod-with-a-hostpath-volume","title":"Step 2: Create a Pod with a <code>hostPath</code> volume","text":"<p>Create a file named <code>hostpath-pod.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-demo\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command: [ \"sh\", \"-c\", \"sleep 3600\" ]\n    volumeMounts:\n    - mountPath: /data\n      name: host-volume\n  volumes:\n  - name: host-volume\n    hostPath:\n      path: /data/hostpath-test\n      type: DirectoryOrCreate\n</code></pre> <p>Apply the Pod:</p> <pre><code>kubectl apply -f hostpath-pod.yaml\n</code></pre>"},{"location":"storage/hostpath/#step-3-interact-with-the-pod","title":"Step 3: Interact with the Pod","text":"<p>Enter the pod and write a file:</p> <pre><code>kubectl exec -it hostpath-demo -- sh\necho \"This is written from the pod\" &gt; /data/hello.txt\nexit\n</code></pre>"},{"location":"storage/hostpath/#step-4-verify-on-the-host","title":"Step 4: Verify on the Host","text":"<p>On the host (e.g. <code>minikube ssh</code>):</p> <pre><code>cat /data/hostpath-test/hello.txt\n</code></pre> <p>You should see:</p> <pre><code>This is written from the pod\n</code></pre>"},{"location":"storage/hostpath/#step-5-optional-test-persistence","title":"Step 5 (Optional): Test persistence","text":"<p>Delete and recreate the Pod:</p> <pre><code>kubectl delete pod hostpath-demo\nkubectl apply -f hostpath-pod.yaml\n</code></pre> <p>Then check again in the pod:</p> <pre><code>kubectl exec -it hostpath-demo -- cat /data/hello.txt\n</code></pre> <p>The file should still be there.</p>"},{"location":"storage/hostpath/#what-have-you-learned","title":"What have You Learned","text":"<ul> <li><code>hostPath</code> allows pods to access host node files/directories.</li> <li>Changes from inside the pod are reflected on the host and vice versa.</li> <li>This is useful for debugging, logs, or interacting with host-mounted devices \u2014 but not recommended for production due to tight coupling with host nodes.</li> </ul>"},{"location":"storage/nfs/","title":"CSI NFS Dynamic Provisioning Lab","text":"<p>Compatible with Minikube, Kind, or any Kubernetes 1.20+</p>"},{"location":"storage/nfs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster</li> <li><code>kubectl</code> installed</li> <li>Linux-based system (for NFS setup)</li> <li>Internet access for pulling container images</li> </ul>"},{"location":"storage/nfs/#step-1-install-configure-nfs-server-on-host-or-vm","title":"Step 1: Install &amp; Configure NFS Server (On Host or VM)","text":"<p>On your host or any reachable machine (e.g. Minikube VM), install NFS server:</p> <pre><code>sudo apt update\nsudo apt install nfs-kernel-server -y\n\nsudo mkdir -p /srv/nfs/kubedata\nsudo chown nobody:nogroup /srv/nfs/kubedata\nsudo chmod 777 /srv/nfs/kubedata\n\necho \"/srv/nfs/kubedata *(rw,sync,no_subtree_check,no_root_squash)\" | sudo tee -a /etc/exports\nsudo exportfs -rav\n</code></pre> <p>Start the service:</p> <pre><code>sudo systemctl enable nfs-server\nsudo systemctl start nfs-server\n</code></pre> <p>\ud83d\udca1 On Minikube, use <code>minikube ssh</code> and perform the above commands inside the VM.</p>"},{"location":"storage/nfs/#step-2-deploy-the-nfs-csi-driver","title":"Step 2: Deploy the NFS CSI Driver","text":"<p>Install the CSI NFS driver (official):</p> <pre><code>kubectl apply -k \"github.com/kubernetes-csi/csi-driver-nfs/deploy/kubernetes/overlays/stable?ref=release-1.6\"\n</code></pre> <p>Verify:</p> <pre><code>kubectl get pods -n kube-system -l app=csi-nfs-controller\nkubectl get daemonset -n kube-system -l app=csi-nfs-node\n</code></pre>"},{"location":"storage/nfs/#step-3-create-a-storageclass","title":"Step 3: Create a StorageClass","text":"<p>Create a file called <code>nfs-sc.yaml</code>:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-csi\nprovisioner: nfs.csi.k8s.io\nparameters:\n  server: &lt;NFS_SERVER_IP&gt;\n  share: /srv/nfs/kubedata\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate\n</code></pre> <p>\ud83d\udd01 Replace <code>&lt;NFS_SERVER_IP&gt;</code> with the IP address of the machine where the NFS server is running.</p> <p>Apply it:</p> <pre><code>kubectl apply -f nfs-sc.yaml\n</code></pre>"},{"location":"storage/nfs/#step-4-create-a-pvc-pod-using-the-nfs-storageclass","title":"Step 4: Create a PVC + Pod using the NFS StorageClass","text":"<p>Create <code>nfs-pvc-pod.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: nfs-csi\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-app\nspec:\n  containers:\n  - name: app\n    image: busybox\n    command: [ \"sh\", \"-c\", \"echo 'Hello from NFS!' &gt; /data/hello.txt &amp;&amp; sleep 3600\" ]\n    volumeMounts:\n    - mountPath: /data\n      name: nfs-vol\n  volumes:\n  - name: nfs-vol\n    persistentVolumeClaim:\n      claimName: nfs-pvc\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f nfs-pvc-pod.yaml\n</code></pre>"},{"location":"storage/nfs/#step-5-verify-nfs-volume-mount-and-file-write","title":"Step 5: Verify NFS Volume Mount and File Write","text":"<p>Check logs or exec into pod:</p> <pre><code>kubectl exec -it nfs-app -- cat /data/hello.txt\n</code></pre> <p>You should see:</p> <pre><code>Hello from NFS!\n</code></pre> <p>Then on the NFS server:</p> <pre><code>cat /srv/nfs/kubedata/*/data/hello.txt\n</code></pre> <p>You'll see the same file \u2014 confirming dynamic provisioning!</p>"},{"location":"storage/nfs/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete -f nfs-pvc-pod.yaml\nkubectl delete storageclass nfs-csi\n</code></pre>"},{"location":"storage/nfs/#what-you-learned","title":"What You Learned","text":"<ul> <li>How to set up an NFS server</li> <li>Install the NFS CSI driver</li> <li>Define a StorageClass that supports dynamic provisioning</li> <li>Use a PVC and automatically provision NFS-backed volumes</li> <li>Verify cross-node shared storage (ReadWriteMany)</li> </ul>"}]}